{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6415e6",
   "metadata": {},
   "source": [
    "# quest 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed5d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection in the context of Principal Component Analysis (PCA) refers to the process of transforming high-dimensional data into a lower-dimensional space. This is done by projecting the original data points onto a new set of axes (principal components) that capture the maximum variance in the data.\n",
    "\n",
    "# How Projection is Used in PCA\n",
    "# Here's a detailed breakdown of how projection is utilized in PCA:\n",
    "\n",
    "# Data Centering:\n",
    "\n",
    "# Step: Subtract the mean of each feature from the data to center it around the origin.\n",
    "# Purpose: Centering ensures that the first principal component explains the maximum variance starting from the mean of the data.\n",
    "# Covariance Matrix Computation:\n",
    "\n",
    "# Step: Compute the covariance matrix of the centered data.\n",
    "# Purpose: The covariance matrix captures the pairwise covariances between the features, reflecting how much they vary together.\n",
    "# Eigen Decomposition:\n",
    "\n",
    "# Step: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "# Purpose: Eigenvalues represent the amount of variance explained by each principal component, and eigenvectors represent the directions of these principal components.\n",
    "# Principal Components Selection:\n",
    "\n",
    "# Step: Select the top \n",
    "# 𝑘\n",
    "# k eigenvectors corresponding to the largest eigenvalues. These eigenvectors form the new axes (principal components).\n",
    "# Purpose: The top \n",
    "# 𝑘\n",
    "# k principal components capture the most variance in the data.\n",
    "# Projection onto Principal Components:\n",
    "\n",
    "# Step: Project the original high-dimensional data onto the new axes (principal components).\n",
    "# Procedure:\n",
    "# Represent the original data as a matrix \n",
    "\n",
    "# X (with rows as data points and columns as features).\n",
    "# Represent the selected \n",
    "\n",
    "# k eigenvectors as a matrix \n",
    "\n",
    "# W (with columns as eigenvectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a63cd",
   "metadata": {},
   "source": [
    "# quest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b537b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Principal Component Analysis (PCA) is fundamentally an optimization problem that seeks to find the directions (principal components) along which the variance of the data is maximized. This is achieved through a series of steps that involve linear algebra and eigenvalue decomposition. Here's a detailed breakdown of how the optimization problem in PCA works and what it aims to achieve:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662dd6b1",
   "metadata": {},
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a64f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works and why it is effective in dimensionality reduction. Here's a detailed explanation of this relationship:\n",
    "\n",
    "# Covariance Matrix\n",
    "# A covariance matrix is a symmetric matrix that summarizes the pairwise covariances between the variables in a dataset. For a dataset with \n",
    "# 𝑛\n",
    "# n observations and \n",
    "# 𝑑\n",
    "# d variables, the covariance matrix \n",
    "# 𝐶\n",
    "\n",
    "# d×d matrix defined as:\n",
    "\n",
    "# 𝐶\n",
    "# =\n",
    "# 1\n",
    "# 𝑛\n",
    "# −\n",
    "# 1\n",
    "# 𝑋\n",
    "# 𝑇\n",
    "# 𝑋\n",
    "# C= \n",
    "# n−1\n",
    "# 1\n",
    "# ​\n",
    "#  X \n",
    "# T\n",
    "#  X\n",
    "# where \n",
    "# 𝑋\n",
    "# X is the centered data matrix (with each row representing an observation and each column representing a variable) and \n",
    "# 1\n",
    "# 𝑛\n",
    "# −\n",
    "# 1\n",
    "# n−1\n",
    "# 1\n",
    "# ​\n",
    "#   is a normalization factor.\n",
    "\n",
    "# Relationship to PCA\n",
    "# PCA aims to find a set of orthogonal axes (principal components) that capture the maximum variance in the data. The principal components are derived from the covariance matrix of the data. Here's how the covariance matrix is related to PCA:\n",
    "\n",
    "# Eigenvectors of the Covariance Matrix:\n",
    "\n",
    "# The eigenvectors of the covariance matrix represent the directions (principal components) along which the data varies the most. These eigenvectors form the basis for the new feature space in PCA.\n",
    "# Eigenvalues of the Covariance Matrix:\n",
    "\n",
    "# The eigenvalues of the covariance matrix indicate the amount of variance explained by each principal component. Larger eigenvalues correspond to directions in which the data varies the most.\n",
    "# Transformation of Data:\n",
    "\n",
    "# PCA involves transforming the original data into a new feature space spanned by the principal components. This transformation is achieved by projecting the data onto the eigenvectors of the covariance matrix.\n",
    "# Variance Explained:\n",
    "\n",
    "# The variance of the data along each principal component is given by the corresponding eigenvalue. PCA selects the principal components with the largest eigenvalues, ensuring that the maximum variance is captured.\n",
    "# Importance in PCA\n",
    "# The covariance matrix plays a crucial role in PCA for the following reasons:\n",
    "\n",
    "# Capturing Data Relationships:\n",
    "\n",
    "# The covariance matrix summarizes the relationships between different variables in the dataset. By analyzing these relationships, PCA identifies the directions in which the data varies the most.\n",
    "# Variance Maximization:\n",
    "\n",
    "# PCA seeks to maximize the variance of the data along the principal components. The covariance matrix provides the information necessary to identify these directions of maximum variance.\n",
    "# Orthogonality of Principal Components:\n",
    "\n",
    "# The eigenvectors of the covariance matrix are orthogonal, meaning that they represent uncorrelated directions in the original feature space. This orthogonality property ensures that the principal components are independent of each other.\n",
    "# Dimensionality Reduction:\n",
    "\n",
    "# PCA selects a subset of principal components based on their corresponding eigenvalues. This subset captures most of the variance in the data, allowing for effective dimensionality reduction while preserving important information.\n",
    "# Summary\n",
    "# In summary, the covariance matrix encapsulates the relationships between variables in a dataset, providing the foundation for PCA. By analyzing the eigenvectors and eigenvalues of the covariance matrix, PCA identifies the principal components that capture the maximum variance in the data, leading to effective dimensionality reduction and feature extraction. The covariance matrix serves as a key component in understanding the structure of the data and deriving meaningful insights through PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcca41",
   "metadata": {},
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8340a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works and why it is effective in dimensionality reduction. Here's a detailed explanation of this relationship:\n",
    "\n",
    "# Covariance Matrix\n",
    "# A covariance matrix is a symmetric matrix that summarizes the pairwise covariances between the variables in a dataset. For a dataset with \n",
    "# 𝑛\n",
    "# n observations and \n",
    "# 𝑑\n",
    "# d variables, the covariance matrix \n",
    "# 𝐶\n",
    "# C is a \n",
    "# 𝑑\n",
    "# ×\n",
    "# 𝑑\n",
    "# d×d matrix defined as:\n",
    "\n",
    "# 𝐶\n",
    "# =\n",
    "# 1\n",
    "# 𝑛\n",
    "# −\n",
    "# 1\n",
    "# 𝑋\n",
    "# 𝑇\n",
    "# 𝑋\n",
    "# C= \n",
    "# n−1\n",
    "# 1\n",
    "# ​\n",
    "#  X \n",
    "# T\n",
    "#  X\n",
    "# where \n",
    "# 𝑋\n",
    "# X is the centered data matrix (with each row representing an observation and each column representing a variable) and \n",
    "# 1\n",
    "# 𝑛\n",
    "# −\n",
    "# 1\n",
    "# n−1\n",
    "# 1\n",
    "# ​\n",
    "#   is a normalization factor.\n",
    "\n",
    "# Relationship to PCA\n",
    "# PCA aims to find a set of orthogonal axes (principal components) that capture the maximum variance in the data. The principal components are derived from the covariance matrix of the data. Here's how the covariance matrix is related to PCA:\n",
    "\n",
    "# Eigenvectors of the Covariance Matrix:\n",
    "\n",
    "# The eigenvectors of the covariance matrix represent the directions (principal components) along which the data varies the most. These eigenvectors form the basis for the new feature space in PCA.\n",
    "# Eigenvalues of the Covariance Matrix:\n",
    "\n",
    "# The eigenvalues of the covariance matrix indicate the amount of variance explained by each principal component. Larger eigenvalues correspond to directions in which the data varies the most.\n",
    "# Transformation of Data:\n",
    "\n",
    "# PCA involves transforming the original data into a new feature space spanned by the principal components. This transformation is achieved by projecting the data onto the eigenvectors of the covariance matrix.\n",
    "# Variance Explained:\n",
    "\n",
    "# The variance of the data along each principal component is given by the corresponding eigenvalue. PCA selects the principal components with the largest eigenvalues, ensuring that the maximum variance is captured.\n",
    "# Importance in PCA\n",
    "# The covariance matrix plays a crucial role in PCA for the following reasons:\n",
    "\n",
    "# Capturing Data Relationships:\n",
    "\n",
    "# The covariance matrix summarizes the relationships between different variables in the dataset. By analyzing these relationships, PCA identifies the directions in which the data varies the most.\n",
    "# Variance Maximization:\n",
    "\n",
    "# PCA seeks to maximize the variance of the data along the principal components. The covariance matrix provides the information necessary to identify these directions of maximum variance.\n",
    "# Orthogonality of Principal Components:\n",
    "\n",
    "# The eigenvectors of the covariance matrix are orthogonal, meaning that they represent uncorrelated directions in the original feature space. This orthogonality property ensures that the principal components are independent of each other.\n",
    "# Dimensionality Reduction:\n",
    "\n",
    "# PCA selects a subset of principal components based on their corresponding eigenvalues. This subset captures most of the variance in the data, allowing for effective dimensionality reduction while preserving important information.\n",
    "# Summary\n",
    "# In summary, the covariance matrix encapsulates the relationships between variables in a dataset, providing the foundation for PCA. By analyzing the eigenvectors and eigenvalues of the covariance matrix, PCA identifies the principal components that capture the maximum variance in the data, leading to effective dimensionality reduction and feature extraction. The covariance matrix serves as a key component in understanding the structure of the data and deriving meaningful insights through PCA.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Q4. How does the choice of number of principal components impact the performance of PCA?\n",
    "# ChatGPT\n",
    "# The choice of the number of principal components (PCs) in Principal Component Analysis (PCA) significantly impacts the performance and effectiveness of PCA in several ways:\n",
    "\n",
    "# 1. Amount of Variance Retained\n",
    "# Higher Number of PCs:\n",
    "\n",
    "# Retains more variance in the data, as each additional PC captures additional variance.\n",
    "# Results in a more faithful representation of the original data but may also retain noise and irrelevant information.\n",
    "# Lower Number of PCs:\n",
    "\n",
    "# Retains less variance in the data, leading to a more compressed representation.\n",
    "# May discard some useful information but reduces the risk of overfitting and emphasizes the most significant patterns.\n",
    "# 2. Dimensionality Reduction\n",
    "# Higher Number of PCs:\n",
    "\n",
    "# Leads to a higher-dimensional representation of the data in the transformed space.\n",
    "# May be necessary to capture fine-grained variations in complex datasets but can also lead to increased computational complexity and overfitting.\n",
    "# Lower Number of PCs:\n",
    "\n",
    "# Results in a lower-dimensional representation of the data, reducing computational complexity and memory requirements.\n",
    "# Simplifies the model and may enhance interpretability but may also lose some information critical for accurate modeling.\n",
    "# 3. Computational Efficiency\n",
    "# Higher Number of PCs:\n",
    "\n",
    "# Increases computational complexity, as more PCs require more computation for projection and reconstruction.\n",
    "# May lead to longer processing times and higher memory usage, especially for large datasets.\n",
    "# Lower Number of PCs:\n",
    "\n",
    "# Decreases computational complexity, making PCA faster and more efficient.\n",
    "# Reduces memory requirements and may enable the use of PCA in resource-constrained environments.\n",
    "# 4. Overfitting and Underfitting\n",
    "# Higher Number of PCs:\n",
    "\n",
    "# Increases the risk of overfitting, especially if the dataset contains noise or irrelevant features.\n",
    "# May result in a model that captures too much of the training data's variability and fails to generalize well to new data.\n",
    "# Lower Number of PCs:\n",
    "\n",
    "# Reduces the risk of overfitting by focusing on the most significant patterns in the data.\n",
    "# However, too few PCs may lead to underfitting, where the model fails to capture important variations in the data.\n",
    "# 5. Model Performance\n",
    "# Higher Number of PCs:\n",
    "\n",
    "# May lead to better performance on training data, as more variance is retained.\n",
    "# However, performance on test data may suffer due to overfitting, resulting in poor generalization.\n",
    "# Lower Number of PCs:\n",
    "\n",
    "# May result in simpler models that generalize better to new data, especially if the dataset is noisy or contains irrelevant features.\n",
    "# However, performance may be suboptimal if too few PCs are selected, leading to underfitting.\n",
    "# Choosing the Number of Principal Components\n",
    "# Scree Plot: Visualize the eigenvalues of the covariance matrix and select the number of PCs corresponding to the \"elbow\" point, where the eigenvalues start to level off.\n",
    "\n",
    "# Cumulative Variance: Select the number of PCs that explain a predetermined percentage of the total variance (e.g., 90% or 95%).\n",
    "\n",
    "# Cross-Validation: Use cross-validation to evaluate model performance for different numbers of PCs and select the number that optimizes performance on a validation set.\n",
    "\n",
    "# Domain Knowledge: Consider the specific characteristics of the dataset and the goals of the analysis when choosing the number of PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef6ab5",
   "metadata": {},
   "source": [
    "# quest 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c4ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA can be effectively used for feature selection by leveraging its ability to capture the most significant patterns and variations in the data. Here's how PCA can be utilized for feature selection and the benefits of using it for this purpose:\n",
    "\n",
    "# PCA for Feature Selection:\n",
    "# Dimensionality Reduction:\n",
    "\n",
    "# PCA transforms the original high-dimensional feature space into a lower-dimensional space spanned by the principal components.\n",
    "# By selecting a subset of the principal components that capture the most variance, PCA effectively reduces the dimensionality of the dataset.\n",
    "# Feature Ranking:\n",
    "\n",
    "# PCA ranks the original features based on their contribution to the principal components.\n",
    "# Features that have a higher impact on the principal components (i.e., higher loadings) are considered more important and are retained for further analysis.\n",
    "# Thresholding:\n",
    "\n",
    "# PCA allows for setting a threshold on the variance explained by the principal components.\n",
    "# Features that contribute less to the overall variance can be discarded, resulting in a more concise feature set.\n",
    "# Transformation:\n",
    "\n",
    "# After selecting the relevant principal components, the original features can be transformed into the new lower-dimensional space.\n",
    "# This transformation provides a reduced set of features that retains most of the information present in the original dataset.\n",
    "# Benefits of Using PCA for Feature Selection:\n",
    "# Dimensionality Reduction:\n",
    "\n",
    "# PCA reduces the number of features while preserving the essential information in the data.\n",
    "# This simplifies the dataset, making it more manageable and reducing the risk of overfitting in subsequent modeling tasks.\n",
    "# Noise Reduction:\n",
    "\n",
    "# PCA filters out noise and irrelevant information by focusing on the principal components that capture the most variance.\n",
    "# This results in a cleaner and more informative feature set, leading to improved model performance.\n",
    "# Multicollinearity Mitigation:\n",
    "\n",
    "# PCA eliminates multicollinearity (correlation between features) by transforming the original features into a set of orthogonal principal components.\n",
    "# This reduces redundancy in the feature set and prevents issues associated with multicollinearity in predictive modeling.\n",
    "# Interpretability:\n",
    "\n",
    "# PCA provides a concise representation of the data by expressing it in terms of a smaller set of principal components.\n",
    "# This enhances the interpretability of the feature set, making it easier to understand the underlying structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b402379",
   "metadata": {},
   "source": [
    "# question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7da8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis (PCA) finds application in various domains within data science and machine learning. Here are some common applications:\n",
    "\n",
    "# 1. Dimensionality Reduction:\n",
    "# High-Dimensional Data: PCA is widely used to reduce the dimensionality of datasets with a large number of features.\n",
    "# Image Compression: PCA can compress images by representing them in a lower-dimensional space while retaining most of the important information.\n",
    "# Text Mining: In text analysis, PCA can reduce the dimensionality of document-term matrices, improving computational efficiency and reducing noise.\n",
    "# Spectral Data Analysis: PCA is used to reduce the dimensionality of spectral data in fields such as chemistry and remote sensing.\n",
    "# 2. Exploratory Data Analysis (EDA):\n",
    "# Data Visualization: PCA helps visualize high-dimensional data by projecting it onto lower-dimensional spaces that can be easily visualized.\n",
    "# Clustering Analysis: PCA can be used as a preprocessing step to reduce the dimensionality of data before applying clustering algorithms, aiding in cluster visualization and interpretation.\n",
    "# 3. Feature Engineering and Selection:\n",
    "# Feature Selection: PCA assists in selecting the most relevant features by identifying the principal components that capture the most variance in the data.\n",
    "# Anomaly Detection: PCA helps identify outliers or anomalies by reconstructing data from the reduced set of principal components and comparing it with the original data.\n",
    "# 4. Signal Processing:\n",
    "# Image and Signal Denoising: PCA can be used to denoise images and signals by filtering out noise in lower-dimensional representations.\n",
    "# Speech Recognition: PCA is applied in speech processing to reduce the dimensionality of feature vectors and improve the efficiency of recognition algorithms.\n",
    "# 5. Machine Learning Modeling:\n",
    "# Preprocessing: PCA serves as a preprocessing step to reduce the dimensionality of data before applying machine learning algorithms, improving computational efficiency and reducing overfitting.\n",
    "# Collaborative Filtering: PCA is employed in recommendation systems to reduce the dimensionality of user-item matrices and enhance the efficiency of collaborative filtering algorithms.\n",
    "# 6. Quality Control and Monitoring:\n",
    "# Process Control: PCA is used in industries such as manufacturing and pharmaceuticals for multivariate process monitoring and fault detection.\n",
    "# Biological Data Analysis: PCA helps analyze gene expression data and identify patterns associated with different biological conditions or diseases.\n",
    "# 7. Finance and Economics:\n",
    "# Portfolio Management: PCA aids in portfolio optimization by reducing the dimensionality of financial data and identifying the most important factors influencing asset returns.\n",
    "# Economic Forecasting: PCA is applied in macroeconomic modeling to extract underlying factors from high-dimensional economic datasets and improve forecasting accuracy.\n",
    "# 8. Healthcare and Bioinformatics:\n",
    "# Medical Imaging: PCA is used in medical image analysis for dimensionality reduction, feature extraction, and visualization of complex imaging data.\n",
    "# Drug Discovery: PCA helps analyze high-dimensional molecular data to identify patterns associated with drug response or toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0900a",
   "metadata": {},
   "source": [
    "# question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad26e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In the context of Principal Component Analysis (PCA), \"spread\" and \"variance\" are closely related concepts that refer to the distribution of data along different dimensions or axes. Here's how they are related:\n",
    "\n",
    "# Spread:\n",
    "# Spread generally refers to the extent or distribution of data points along a particular axis or direction.\n",
    "# In PCA, spread can be understood as how widely the data points are distributed along each principal component axis.\n",
    "# Variance:\n",
    "# Variance quantifies the dispersion or variability of data points around the mean.\n",
    "# In PCA, variance is a measure of how much information or variability is captured by each principal component.\n",
    "# Relationship:\n",
    "# Variance Explained by Principal Components:\n",
    "\n",
    "# In PCA, each principal component captures a certain amount of variance in the original dataset.\n",
    "# The first principal component captures the maximum variance, followed by subsequent components capturing decreasing amounts of variance.\n",
    "# Spread along Principal Component Axes:\n",
    "\n",
    "# The spread of data points along each principal component axis reflects the variance captured by that component.\n",
    "# Higher spread along a principal component axis indicates higher variance captured by that component.\n",
    "# Eigenvalues and Spread:\n",
    "\n",
    "# In PCA, the eigenvalues of the covariance matrix represent the variance explained by each principal component.\n",
    "# Larger eigenvalues indicate higher spread or variability along the corresponding principal component axis.\n",
    "# Dimensionality Reduction and Spread:\n",
    "\n",
    "# PCA aims to reduce the dimensionality of the dataset while retaining as much variance (information) as possible.\n",
    "# By selecting a subset of principal components that capture the most variance, PCA effectively reduces the spread of data points in the transformed space while preserving important patterns and variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39a327",
   "metadata": {},
   "source": [
    "# question8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5e58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA uses the spread and variance of the data to identify principal components through eigenvalue decomposition of the covariance matrix. Here's how it works:\n",
    "\n",
    "# 1. Spread and Variance:\n",
    "# Spread: Refers to the distribution of data points along different axes or dimensions.\n",
    "# Variance: Quantifies the dispersion or variability of data points around the mean along each axis.\n",
    "# 2. Covariance Matrix:\n",
    "# PCA begins by computing the covariance matrix of the centered data.\n",
    "# The covariance matrix summarizes the relationships between different features in the dataset and provides information about the spread and variance of the data along different dimensions.\n",
    "# 3. Eigenvalue Decomposition:\n",
    "# PCA performs eigenvalue decomposition of the covariance matrix to find its eigenvectors and eigenvalues.\n",
    "# Each eigenvector represents a principal component, and its corresponding eigenvalue represents the amount of variance explained by that component.\n",
    "# 4. Selection of Principal Components:\n",
    "# PCA selects the principal components based on the eigenvalues of the covariance matrix.\n",
    "# Principal components with higher eigenvalues capture more variance in the data and are considered more important.\n",
    "# PCA typically sorts the eigenvalues in descending order and selects the top \n",
    "# 𝑘\n",
    "# k eigenvectors corresponding to the highest eigenvalues, where \n",
    "# 𝑘\n",
    "# k is the desired number of principal components.\n",
    "# 5. Projection of Data:\n",
    "# After selecting the principal components, PCA projects the original data onto these components.\n",
    "# This transformation results in a lower-dimensional representation of the data that preserves the most important patterns and variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1769cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
